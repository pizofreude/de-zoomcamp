 Hi everyone, welcome to the workshop on DLT on the Dating Russian. Today we will learn about the tool called DLT and VLT is our instructor today. But before we start, I just want to quickly show you around. I want to show you where the things are. This is our repo, the course repository. Then you scroll to the cohorts folder and then you find the 2025 cohorts folder. And the workshop slash DLT. This is where all the materials are. So that's one thing. So there is a home work. So this is the home work that you will need to do after the workshop. And also we have a four on our course management platform. I don't know why everything is so slow for me today. But for the home work, when you go to Datings and Zoom camp and in session with DLT, workshop one, this is where you put answers to the home work. And that's all from my site. Let's welcome VLT and VLT that will be our instructor for today. And I'm really looking forward to the workshop today. Yay, thank you. Thank you, I'll see. So first, let me introduce myself. Hi, everyone. My name is VLT. I'm a solutions engineer in the company called DLT Hub, which is a main creator of library open source library DLT. So as a solution engineer, what I mostly do, I'm talking to our users and to our customers, understanding their use cases and help them to implement solutions with our library DLT. I also advise them on best practices, how to overcome some issues. So I got to say I saw a lot of different Datings and setups all the variety of them. So today I will try to cover the Datings in session part. Let me share my screen. Okay, hope you can see it. So yeah, I'll say already told you that there is like a redmeat.md, where we can find all the information about the today workshop, including the homework. So please, after workshop is done, do some homework and submit it because at the, if you go there, at the end you will find this form for submitting and with a deadline, by the way, which is some DLT library. So hopefully everything will be great. So today, what we're covering today? As I said, the main topic is Datings, with DLT. And in this workshop, you will learn how to build robust scalable and self-managing data pipelines. You will learn something about best practices, including incremental loading and we will learn how to not only load data to warehouse, but also to data link. I would share everything. But, you have to do, we need to prepare anything any environment for this workshop, or just call up Google, call up node, look that you have in front of us. Yeah, Google, call up Google could be enough. The only thing is that at some point, I will be loading data to be query and to do this, you need to have some credentials of the query. So, but everything else you can do. Okay. So, yeah, let's start with what's Datings is. So, just for a bit of context, actually my background is not Datings here. My background is machine learning related. I started as a data scientist. I've built some machine learning pipelines, to read some neural networks and stuff. And for me, data usually appeared at this, some kind of data warehouse, SQL database, or maybe it was like, pocket files or CSV files. And I haven't even thought about how it happened to be there. So, for me, kind of data sets were magically appeared at the destination. But in reality, the data usually is cut out across like every possible source you can imagine. It could be API, it could be a lot of databases, it could be files like somewhere in Google Drive, so I feel like this. And the work of Datings in here is to consolidate all this data, make them clear, make them readable and normalized for data scientists, for data analytics, and for all the data workflows. So, in general, I would separate all the data into groups. It could be a well structured data, which has explicit schema, it can be used immediately, usually, some kind of pocket files, a raw, or if you have tables in database, it's also both structured data. But in most of the cases, unfortunately, the data is unstructured or quickly structured. For example, it could be JSON files where you know what the types of all the fields, it could be nested JSON files, some data is inconsistent or missing in the worst case scenario, it's even like PDF files. So, like as a Datings in here, your job is to be this magician and to consolidate all this information into a warehouse. So, usually how you do it, you build data pipelines. And data pipelines is like the backbone of any modern organization. And the main goal is to transform all these like roles, cut-or-data into some actionable insights. So, if you look at this schema, so there are a lot of different ways how you can define data by play, but this is one of them. And here you can see the data pipeline happen in five steps, which is collect data, ingest, store, compute, and consume. As a Datings year, we mostly focused on the first three collection, gestant store, but other data workflows is also our responsibility in Datings years. So, let me go through each of them just very briefly, just to give you an overview. So, first one is collect. As I mentioned, data usually located in like a lot of different sources. For example, if you have some websites, it's usually collected in the database behind this website. It could be different APIs. It could be some serums like hopsport or zendes, so other different things. And the first step is to collect data from their electric structured. The second step is ingest data. And depending on if you are dealing with both structural data from like a skill databases, it could be less work. But if it's JSON files, especially from APIs, it could be a lot of different techniques for normalization, for cleaning, for flattening the data, and a lot of work. So, once you did all of this, use towards the data and some kind of storage. So, here you can see three different types of them. It's a data lake, data warehouse, and data lake apps. What's a difference? So, in data lake, data usually stores as files, and most of the case is pocket-lify. And the advantage of this storing is that you can store a massive data like this, like a terabytes of data, and it's quite cheap. But the disadvantage is that when you need to access data quickly and to search for something, it could take some time. The very house one, probably what you have familiar with, it could be some kind of like BigQuery storage, Snowflake, Redshift, it could be even like my square database with a couple of tables. And the lake house is kind of a mix with a buff. So, you have data storage in files, but you have additional layer of metadata, which allow you to search data very quickly. Yeah. So, once the data is some kind of their house, some kind of commonplace, then it could be a process further to consumers. There are two types of compute here, it's batch processing, where you process data Chengmichang or streaming when you just process data recovery. And finally, at the last stage, we have consume where our data scientists or other lists were like a million years building their software in Dutch boards or trading machine models were building reports and stuff like this. So, we get actionable insights from data. And actually, data in here is responsible for all of this part of this pipeline. And why is the engine UI so important is because they don't just like build the pipelines, the action need to make sure that pipeline is reliable, efficient and scalable. So, several different aspects, which you need to pay attention. First one is you need to be careful about data storage and try to opt my costs because usually it's not cheap. You need to keep costs low and the phone's high. So, that is why you decide between like do you want to keep data in their house? Do you want to keep data in data like? So, it's up to you. Also, data here should ensure that data are all consistent, that data has a good quality. No duplicates there, no missing values because data analysts are lying on you to get like good quality data. Of course, important thing, important topic is the governance, especially if we're talking about like big enterprises or big companies. So, all the data have workflows, should be compliant with all the regulations, with Jiuq-Biah. And you need to manage who get access to each data. You need to make sure that no extensive data is exposed, like no credit card number or like maybe personal addresses, personal phone, everything need to be well matched. And of course, you need to keep in mind that everything is changing. So, the data by blinds are not set in stone because if your company is doing well, you probably at some points, you'll need to scale your blinds. So, when you're building with your architecture, just ask yourself, okay, now I have like 10 different sources of data. What if I would have 1000 sources of data? Would it be scale so easily or not? Yeah, and during this workshop, we will talk about how to build, as I said, robust, scalable, seffertaining by blinds. We of course cannot like talk about all possible aspects of this, but we'll cover a lot. So, this is the end of intro. Is there any questions? There is one, maybe you can quickly answer what's the difference between DBT and DLT? Oh yeah, I got this question a lot. So, DLT is now for data load 2, and this is a library for data ingestion. While DBT is a transformation tool. So, with DBT, you write your SQL queries and performance transformation. So, it basically two different tools. And if you scroll up where you have the diagram, I think, on this diagram, DLT and DBT are in different areas, right? Yeah, exactly. So, DLT will cover the collecting gestions tour part, while DBT will probably happen somewhere in consume, because the DBT will be used by data analysts to like perform their joys, their starskimas, and stuff like this. Yeah, thank you. Okay, cool. But it's a good question. Every time I say, hey, have you heard about DLT? They're like, mm, DBT? No, no, no, DLT is different, but it's also three letters. Okay, yeah. So, as I said before, we will cover most of this collecting gestions tour part. You may hear it as of dating gestion, or you may hear it like this term called ETA, which extracts transform load, but it's all the same. The AD behind this is that it's happening in three steps. First, you extract the data, then you normalize like clean, transformed, defines kimas, and then you load the data. DLT also works with these three steps. So, first of all, is extracting data. So, as a dating year, 99% of your data would be probably stored in either SQL databases, APIs, or files. And in this workshop, we will cover the APIs, which is weekly structure data, and probably is the most complex ones, but unfortunately, they also the most common ones. So, API is kind of a don't wait to the data. I'm sure most of you know what it is, but I will just like quickly give you an overview. So, there are like three common types of APIs. First one is resplapize, meresity guys, which provides your records of data from unilism kind of business applications, from for example, serum system, or I don't know, there's APIs for everything. For example, if you listen to music in Spotify, there is an API recipe, I've also Spotify, you can check how many different genres you listen to last year. Or if you want to know what other most popular movies there's an API for movie-db, there's API for Pokémon, there's an API for anything. The second one is file-based API, which usually returns some kind of bulk data in JSON or market formats. And the third one is database eight days, because in some cases database, for example, like one good DBOS query is put behind the API as well. So, as a dating year, you will probably work with all of these types, and what they have in common is challenges. So, what you need to keep in mind to run like everything smoothly. First thing is hardware limits. So, we need to be mindful of what memory you're using, RAM, and what storage you're using as in disk space. And if you overload it, you might crash the whole system. Because at any system you're running your pipelines, it has it to constraints. For example, for Google Collop, we can see what type of RAM can I use. I can use up to 12 gigabytes of RAM and up to 100 gigabytes of disk space. And if you are used to much, they go up full crash, unfortunately. So, the second thing is network reliability, because network fails in a lot of time, and not all of the requests you may to API would be successful. So, you need to take care of all the retries and make a bad lines more robust. The good thing is that a lot of tools have some things like network reliability and hardware limits in place, and ready, like DLT for example, but we will cover this later. The first thing you need to think about is rate limits of APIs, because in a lot of cases, APIs just restrict the number of requests you can do over a time frame. And if you make too many requests, it will just fail, and sometimes even they can block here. And most of the cases you can check the rate limits in API documentation, but the bad news about APIs is that an orphan documentation is not very comprehensive. So, in some cases, it's just like guessing it. So, there are like other challenges like pagination and authentication, and we will cover and explore how to handle them with rest APIs. Okay. So, yeah, rest APIs. It stands for a presentational state transfer, APIs, and APIs stands for application programming interface. This is one of the common most common way to install it, external data. So, rest API allows you to get data through HTTP requests. So, it's usually get post-putted elites when we're talking about getting instruction data, it's usually get. And the hard part is there is no common way out of design and API. So, each of them is a little bit unique in a way, and you would need to find out it yourself. Usually, when we get the data, we use the request library from Python. For example, if we want to get GitHub, right, to GitHub has it on API, and if we want to get the least of all the repository repositories of GitHub, we can just request this URL URL and get the information here. And if we are too lazy, we can just copy past it to some kind of search and get the all the rebels. But it must APIs, it's not that easy because this is a publicly available information, but in most APIs it would be behind the authentication. So, how we work in a little bit more complicated scenarios. This we need to pay attention to these four challenges which are right limits pagination, memory management, and authentication. Let's start with right limits. So, as I said before, a lot of APIs limit the number of requests you can make to prevent kind of overloading. And if you exceed this limit, then API may reject your request, or you need block you for some period of time. So, what you need to do is first of all monitor the API retllimates. Sometimes you can, again, just look at the documentation and learn what is the limit. Sometimes you will get the information in the response in the header, what is the limit, and before you just make another request, you need to pause and wait until the load is renewed. And of course, you need to implement automatic new drives because at some point, you will get a failed request and you need to like wait a little bit and try again. Also, some APIs provide like retry after header, which tells you how, like, how many, how long you have to wait until you can make another request. And so, basically, you need to check like a bit of documentation. How the API is designed. The second thing you need to worry about is authentication. So, the example I showed you with GitHub repo is public information, right? So, I can get access to it from any user. But, for example, if I want to check the HHSH case and then try to do it, whatever it will get is requires authentication. Because this information is not public, this information could only be available to me to myself. So, they added some kind of authentication. There are usually three types of them. The first one is API keys. It's just like a simple toolkit, some number of digit. You usually include in request header or in URL. More complex one is old tokens. It's more secure authentication method. And the third one is basic authentication. So, when you use username and password, let's come on today, but still, yeah, it's important thing that you never should share your API token publicly. And especially, you don't have like never put it into your code. So, if you want to put somewhere like API key, API token, just put it either environment variable or in case of callup, you have this same code keys here. Now, you can see I have my credentials here. You can put it and then use it inside callup by this like user data get. Okay. Good. So, the authentication will cover it and the next wing is pagination. So, in most almost all the cases, IPI does not return all the data at once to you. Usually, they return data in chunks or pages because they could be like just a lot of data. And to retrieve everything, you need to like iterate over these pages. So, let's get a handset a little bit dirty. Let's try to code a little bit. And for the purpose of this workshop, we prepare this API, which is like, we prepare it specifically for this workshop, it's not a real one. So, this API serves the data from New York TechC dataset. Probably, if you ever tried any workshop about data science, you heard of this and how this API is designed. First of all, is does not require any authentication because this is out of the scope of this, but it do gives you data in pages. So, each page have 1000 records and how the pagination is designed, very easy. Then there is no data, the API returns empty page. So, here is the API URL. We can we can check it if you want. Okay, so like from Zoom. Yeah, so we can pass this URL and check, for example, page number one. Yeah, we can see the data here. And if we go to page number 100, okay, no. We will see that the API gives us emptilist. This means that there is no data at the page 100. I think there is like 10 page of data, I think. Okay, so the only parameter, this API takes is the number of page defaults to one. So, there is no page number zero. It starts with one. And yeah, let's try and retrieve the data. So, we'll be using Python. We'll be using the request library. I don't know how well you familiar with it, but it basically gives you several different methods to do HTTP requests. All of them I implemented. I get put, delete, post, etc. So, what we're doing now, we have the loop. We put the parameter, the page number, which turns with one. Then we request the data from these base URL and passing the parameters. When we get the response, we put it in JSON format. And the important thing, if there is no data, then we stop the loop. Right. So, in order to keep it like another bit faster, here in this example, we will stop the loop after two pages. Let's see if it's false. Yep. So, we are getting our records. We can see, so each of this page contains one thousand records. Yeah, and we got basically two pages. Yeah, cool. So, one important note here is that every API designed the pagination differently. Sometimes you will get in like a hudder, the URL to the next page. Sometimes you will get like additional fields, which says next, if there is next page on it. So, you need to be careful. Sounds like a muse of sets, corsairs, tokens, etc. Yeah. Yeah, I'm just choose the correct method. Okay. The next challenge, if what one, is to avoid memory issues during extraction. So, similarly, how API does not give you all the data right away. It instead gives you data in pages. Sillers here. So, you don't want to load all the data in memory at once. Usually a lot of pipelines run with like limited memory. For example, if you run in web life on serverless functions or AWS, Lambda functions or even shared cluster, the memory would be very limited. So, even in the disk space can become like an issue, if you store like a large, very large amount of data. So, the solution for this is batch process ink, of course, all extreme data, if you prefer. So, you will process data in small chunks rather loading everything at once. And this is kind of a balance between memory, usage and throughput. I need to find out how big the chunks should be. You probably don't want to like, process data record by record, but maybe page by page, for example, with APIs. Let's try to do an example here. Again, with the same API. I don't know, if you're following the Google call up with me and running cells, this is wonderful. Let me know if you need more time and I will pause and some of the cells. But yeah, the API is the same. So, again, we have like a URL, method getable. We'll give you give us the data. Again, we have a pagination with page starting from one. Now what we're going to do, if we go to implement the generator. So, each time we will yield on your one page of the data. So, most of the calls is the same. So, again, we have like parameters, number of pages, then we have a request for our base URL, passing the number of page, getting response, and then we yielding this page, upgrade the page number, and yeah, etc. So, when we're using it, using this generator in a loop, each time in memory will be only one page of data, which is very nice, for, of course, memory management. In this case, in this example, we will break after first one, just to keep it shorter. But yeah, if we iterate over all of the pages, we will give, we will get every page until there's no page left. So, what we need to pay intention to here is that this approach, which generators, gives us easy memory management. So, we kind of like return page of the data each time. But again, it gives us lower throughput, because like each time we need to wait until we can make next request, like we need to wait for response, because it could take some time, and to simplify all these things, the best practice is to use some kind of tool, which already have all of this implemented. Of course, I'm talking about the OT, which is data-load tool. So, because the OT has a lot of different best practices implemented inside, so it will keep your memory usage low, it will leverage brilliance, with threats and processes, and gives you better performance overall. Yeah, so before this, we were like making all the extraction kind of manually, with only a repository. But starting from this point, I will show you how to do it with DLT. So, with DLT, we'll take care of all the challenges I was talking about, but the nation, right limits, authentication, error handling, and avoiding memory issues. So, what's the DLT data-load tool? This is an open source, by the library, which has a lot of different data-nearing best practices implemented, some of them I mentioned before, but there are a lot more than this. And it also works in free steps, extract, normalize, and load. And with DLT, you can extract data from almost any type of source. I mentioned as good as a basis, you can use it. Like any CRMs, I don't know, HubSport, Google Analytics, Google Shades, NoShand, GitHub, API, and most important, it has a really good connector for any rest API based. Data source. And I also can load data into different destination types. Some of the mostly common use one is snowflake, or BigQuery, or DougDB, or basically anything to work with. And the good thing about it, it's very easy to start working with it, because you just need to do PIP and Style. As I mentioned, it's totally open source, so you can use it in your pet projects, in your Google project, or in your actual work. And a place you can even build a tool on top of DLT if you want. It's all about. Okay, let's just try to use it. First thing, first, we need to install DLT. Well, as I said, we just do PIP and Style. And here you can see that IP can Style, it's not just DLT, but with this modification, our code, DLT. There is in behind this is that I would want to load all my data to DLT. So those of you who don't know, DLT is like a lightweight database, which can be spin up in memory, or in your local disk, and it's just like easy to work with. But instead of DLT, you can have here, PostgreSQL, MySQL, BigQuery, basically anything. I will show you how to change it in the future. Okay, so once the library is installed, I cheated a little bit, I started before the workshop. So it was very fast. But what you can do here is we, let's take our previous code and change it a little bit with DLT. So DLT has this rest client, which helps you to basically implement and overcome all the changes I've talked about before. How you use it? So you need to internalize this rest API client with base URL of your API. This is the same which we used for our Europe taxi, taxi, API. Then you can define the pagination strategy. Here we have simple page number pagination. Because page number is right, one to three, four, and success rate. Base page start was one and total pass is none. What does it mean? And this means that there is no total count of the pages, but instead if there are no pages left, it will be just empty. Okay, so this is the code for in analyzing color client. And then if we want to paginate over some specific endpoint, in this case is dating dreamsome camp API. To remind you, we used it before to get the data. So it was like the base URL first and then the actual black shell slug next. So we put it here and then use the method called paginate. And then this pagination is actually a generator like we built before. So we can we can iterate over it in the loop. For example here, for page in the client. paginate, you'll page. And then again, let's break after first because we careful about our time and see if it works. Hopefully works. So yeah, yeah, we get also one page of data, so 1000 records. Just to like reiterate how duties implies here, extraction from API. First of all, you don't need to manually iterate over pages and skip track of like page numbers and stuff like this. It's also take care of the memory usage because it's all built with generators and data streamed like chunk by chunk, avoiding ram overflows. Inside underneath, it's also handles all the rate limits and retry. So if there would be like an error, a deal table to care of it and retry after some time, it's also like flexible destination support. You can use it with almost any destination. The good thing here as well is that in a lot of cases, rest client can actually detect the paginator itself. Also the authentication, it can detect itself. Not in all of the cases as I said, like everyone designing API rest APIs differently, but in a lot of like common use cases. Okay, so this was like extracting of the data from APIs, we get them, we have records and now we will be moving to normalization. At this point, any questions? So there was a question, what's the difference between Kafka and DLT? And I think you partly covered that and also your colleagues from DLT who are answering questions in live chat, thanks for doing that. But yeah, you showed this slide exactly where Kafka is one of the sources, but maybe you can tell us more about that. Yeah, exactly. So Kafka also can be used for like, let's say, data ingestion, but in a very simple way, where you put some record to it, and then you take a record out of the Kafka topic. DLT does much more than that because it has a lot of different stuff implemented underneath. For example, accessing the rest API, what I just showed, but it also like schema management schema evolution, I will be covering this later a bit. So Kafka is just a tool for different stuff, then DLT, but you can use Kafka as a source. So you can use DLT to connect to your Kafka topic and just load the data in bulk from it and put it in some kind of destination. And what if I wanted to extract data from NAPI to an SESV file or parket file, can I use DLT for that? Yes, definitely. So as you see, part of our destination is also like Amazon S3 or Azure or just local file systems, so you can extract data from API to your local CC files or parket files or different files if you want, what's the reason? I don't know, you may want it. Okay. And there was another question. I think, yeah, like you talked about retail mechanism, so you handle things like exponential back off and these sort of things, right? Yes. So yeah, in a lot of APIs, they provide like root right after field in the headers, DLT can read all of that and protest it as well. And yeah, if there is no like retry after, it will do like exponential retries. And then I like what you just said. Just one thing that you can also configure all of this. So if you want, you can configure how many retries you have, but in most of the case, you just use default. Yeah, another question, I just so we talked about extracting data from API and then putting it to parket or S3 or whatever, parket on S3 or local system. Yeah, but it cannot be, can I use Google Cloud Storage bucket as a source? I'm assuming a parket file somewhere and right to be query from the parket file. Yes. Yes. So any possible combination of sources in destination, if you data in parket file in GCP or in Azure or even in, I don't know, SharePoint Drive, CC file, SharePoint Drive. You can extract it with DLT and put in BigQuery or I don't know, MotherDark or Athena. So any combination of sources in this nation works? And this is actually quite useful because right now we are also on module 3, which is a data warehousing model module. And there in your homework, you need to take a bunch of parket files and put them to BigQuery. And actually you can use DLT for that and it would be like just a bunch of lines, right? Like 10 lines or something. Yes. Exactly. It would be really easy. So for example, to get more information, what I advise you to do is to go to our documentation. So here you can see the full list of all sources. And as I mentioned before, the three of the main is Rust APIs, SQL databases and Cloud Storage and Fast System. And the case you were describing is covered by this one. And also you can see here all the destinations, including BigQuery and stuff. But yeah, basically you could go here and just like pass system source and copy past the code and then you got your data. And I see like there are more questions. Can DLT load data from x and write them to y? How can we find the answer to that if we want? Yeah, just go to our documentation, check sources if there is an ready to use source and destination. And then you have your answer. Moreover, DLT is not only like half all the catalogues of connectors ready to use. It's also a development tool. So you basically can build any type of source. We actually have a full library called Verified Sources where you can see all the sources built by us or our users who contributed this source. So you can see here Google Ads, you can see here like Mail in box, GRA, I don't know, notion anything. So it's really easy with DLT to build your own custom source. And actually like we have something to stick about this and our community building more than two thousand sources every month. So it's very easy, just write. Okay, cool. So we cover the it's tracked step or like the collect of the data step. Now probably the most difficult part is normalizing the data. Um, just here. Okay, so during this step, we usually clean the data. But what does this actually mean? Unulated walls like two steps. First one is normalizing the data. So you need to put some kind of schema behind the data structure, standardize it, was out like the change changing of the actual data. And the second one is filtering data for specific use case. Sometimes you need to select some data. For example, you need to select the data only for the last year, not poll them. And sometimes it's in a way like changing meaning to their analysis. So we will cover mostly in normalizing data. So the first one, which is yeah, you can call it data cleaning. You can call it transformation, anything. But basically what you need to do is to like perform several tasks, which mostly related to metadata. We should have those tasks. There's still all you need to add types. So for example, especially if we are getting data from APIs, there are in JSON formats. So we don't know like is there a number, it's time stamps or what we think that it's all tricks in most of the cases. And at this stage, we need to actually add types to the data. The second one, sometimes you need to rename columns to ensure like all names follow the standard formats. Also in some warehouses, there are a restriction, how you can actually name your tables and columns. And you also need to take care of this. The third one is probably one of the most important. You need to fill out a nested dictionaries because data from APIs are often in JSON format. They are often really nested sometimes very deeply nested. And you want to put to this like nested structure into a skill database, for example. So you need to fill out some these dictionaries. And the last thing is if you have like lists or race, you also need to unness them. In most of the cases to some children tables, because they cannot be stored like directly in one of the column one of the records. If you ever become fused right now, it's okay because we will look at practical examples. And we will you will get more understanding about that. Yeah, maybe one important question to talk about is that why not use JSON directly. JSON is a very great format for data transfer, but it's not very great for analysis. If you have worked on like data science tasks or managing your tasks, you know that you want your data to be clean in table formats very nice. And JSON does not have like any force schema. So you cannot like tell if the field would even exist JSON. You cannot cross this data. It has inconsistent data types. For example, you can have the same data in different formats like NH can be 25 as a GGS. It could be 25 as a string. And it could bring down the delta applications. Yeah, and also hard to work with because like if you have everything as strings, you need first to transfer it to like timestamps or digits record by record. And JSON is not really easy to access data because in order to access like for example specific column, you actually need to read the whole file and go over record by record and find an extract all the data for specific field. Escal databases are much more easier like crocheting the databases are much more easier to access data. Yeah, and also so if we get some search kind of what I said before already to do like a quick look up, you need to like read the whole file. So again, JSON is very great for dating exchange, but not very great for a analytical use case. That is why dating years need to take this GGS and process them to relational form. So in the case of specific like this example with New York taxi data sets, if you take a look at the each record, they already kind of flattened. So they already kind of easy to work with. But this is only because we prepared it. So let's take a look of what was done with the raw data set. To ensure they have this very nice format, how this data was protest before. So the raw data is actually very nested and doesn't have like any specific data types. So for example, if you look at the coordinates, we have this nested structure with coordinates, then inside it we have start, then inside it we have latitude and longitude. But if you take a look on the records, you will find that we have flattened it to four different fields and latitude and longitude and start latitude and longitude. Then timestamps. So originally timestamps were stored separately date and time. Sometimes it could be a unique timestamp. So sometimes it could be, I don't know, all kind of different things which came into mind of the design and everything. So usually we format them as ISO data type string, which is like a common format and you can see it here. But before it, it was not so committed. And the third thing, actually like the original structure had this passenger field, which includes the list of the passengers. And if we think about this, when we design the data pipelines, we would want this to be a separate table, because it contains the data for the passengers. It's not very suitable for the high-level structure. So yeah, so all this work was done like before that. So now if you go to the API, you will see this nice flattened structure. And imagine during this manually, in order to do this, you would need to take several records from API, look at them, think about how it's better to create a schema. And it will take a lot of time even for one API. But if you have hundreds of them, it's just impossible. So that is why we need tools like the DLT, which perform the normalization basically automatically. So what it does is, first of all, to medically detects the schema, so you don't need to define your column types. It can look at the data and say, okay, this is possibly a timestamp. Then it will add an message on. It handles all the data type conversion. For example, if we need to convert strings to dates or to numbers or to bull ends, deal, you can do it automatically. It also splits lists into a child to other tables. And yeah, one important thing, which we haven't covered yet, but it also supports key revolution. What is that? So your data would be probably never stable because like new fields arriving, data types changing, so if schema or the data is changing, you need to support it. And deal to support this automatically. Let's look at the example. So this is like one of the records, which New York City, Taxi Data Set had before. So you can see here a lot of like nested data and some things which should be like a separate table. And what about deal T? It will normalize this data automatically. How do you do it? So first of all, we import deal T, library. We already installed it. So I'm skipping this part. Then we're creating a pipeline. Inside the pipeline, we put the pipeline name, which can be like anything. Then the destination, as I mentioned before, we'll be using dot db as a destination because it's like lightweight. It will be spun up automatically. You don't need to take care about this yourself because if you don't people install deal T with ducted B, everything is already there. And then I just like the data set name. I use here, Taxi riders, but be creative, use whatever you like. Then I'm writing the pipeline with this data. So the good thing about deal T, you can put anything in the data. In this case, I'm just putting the list. But further along, we will talk about what we can put here as well. And usually it's kind of resource. But we talk about it. Then table name, which would be right and to write this position, I will talk about this later as well. And then run. Let's see. Okay, the data is loaded very quickly, of course, because we only have one record. And we can actually expect the trace if we use the pipeline and access the last trace. Then we can see like all the metadata, I've all developed how time is spent on extract state, how time is spent, how many times spend on normalized stage, and etc. And let's look at the exact data. What deal T, um, uh, det with this, uh, may remind you very nested structure. So we can actually access the data set, which were loaded for this. We just use this data set, method, uh, then the name of the table and DF, because I like pandas, right? Okay, this was quick. So we can see all the columns. Let's actually, okay. Look at columns. So we can see here that columns were automated the unnastered. So for example, for coordinates, we had, um, like start long, longitude and latitude. And we can see here coordinates start longitude, the same for latitude and the same for and so do you see, understand this? You can also see like several columns, which has starts with underscore, underscore, DLT. This is subsystem columns where DLT keeps, um, IDs, which are um, unique. Okay, and also DLT created another child table for passengers. May remind you, we had like several different records here, which I said should be, uh, separate table. So here they are, um, and the separate table, we like to underscore as passengers. And here we can see data from that. So nested structure were flattened, um, into separate columns, least were extracted into children tables. And also, uh, we never, beta-densioned, but timestamps were, uh, converted to correct format. You can see here. So yeah, with DLT, it's very easy to do normalization. You don't need to do like manual transformation. You don't need to take care about what actual formats your database of the destination has. And as I mentioned, DLT handle skimmy solutions. So if I like change something here, if I add a new column for example, DLT will handle this, um, as well. Um, okay. This was the implementation part. Do have questions? Um, there is a question that your colleagues already answered, but because afterwards, we'll not have access to the live chat. I think it would be nice if we talk all, all this. So the question was about enrichment where the DLT can do that. And by that, the example was we already have table, but we want to add an extra column to this table from some other data source. Is the DLT, can DLT do that? And this is the right way of doing that. Or it's better to load the data into separate table and then do the join later using some other tool. Um, it's dependent on the specific use case, but in general, we do not recommend to use several different sources and to load to the same table. It's better to load to the different table and then do join. But with DLT, because like it's a Python code, you basically can do anything you like. And for example, if you want to enrich your data with some additional column, you can also do that. Inside your source, you can maybe we will cover like resource and source with later a little bit, but um, yeah, there is a way to do this. You can just go to the documentation and look for example for map function or just like enrichment of the data set. I'm sure we have like, um, I pay you somewhere here about this. And there was also a question about type inference, like how you figure out what type of sign to each of the columns. Do you have some documentation about that? Yes. So if you go, even though we're taking it's under schema, um, let's see, yeah. So here we have all the data types. So because DLT works with all kind of sources and all kinds of destinations, it has internal data types. So first, DLT transform data from source into internal data types. And then it transforms this internal data types to the data types of the warehouse depending on the warehouse. But it covers like all the usual ones, which is like tax double wool, timestamp, and etc. You can have, you can find information about them all in the documentation. Is there some sort of UI for DLT or you only interact with these for the scripts? So DLT first of all is a library, open source library. That is why we do not have like a lot of built for UI. But what we have is a stream late built in UI kind of. You can, when you load the data, it's like hard to show in the callup because usually you need to all the stream late app on your local laptop. But when you load the data, when you run the pipeline, then you have a command, DLT by blind show. And then it will spin up like a very simple, stream late app. And then as this app you can see you can export the data, you can see the data types, what are the tables, and etc. So this is the only UI we're providing. Okay, we have DLT by blind show. Oh, maybe not somewhere here. Okay. Okay, then let's go to the last step here, which is loading the data. So we're done with extracting, which I'm going to use normalization. And now it's time to load the data to the destination. So let's first see how it would happen without DLT. So without DLT, you need to manually handle the schema validation, the batch processing, all the error handling, and all the retries. Because I mentioned before that network tends to fail. And you're not only should handle retries for the source, but you also should handle retries for the destination. So this process becomes special complex, loading data into data warehouse, especially when it's cloud-based warehouse or data-legs. So let's assume we want to load data from the same API to the ducty-b, but we want to load it manually. What we need to do? So we would import the ducty-b, which actually has the Python client, some databases or warehousing don't even have this. Thanks to WBW, have it. There's really a bit easier. So then we need to connect to the actual database. Then we need to execute the SQL statement, which creates a table. And in this SQL statement, we need to put all the column names with all the data types. Then we have a data. Right, in the same one record, then we need to flatten it manually. And it's okay if we have like six columns, but if we have like hundreds of them, it's of course like almost impossible to make your name. Then we insert the record and at the end we close the connection. Let's try to run it. Hopefully everything should be fine. Oh, it was all. Quick. Nice. So yeah, the data is successful. Load into ducty-b. So it all worked. But you need to pay attention that there is no schema management, no atomatics schema management, because it's all manual. There is no automatic root rice. So in this case, my WB is at the same machine where I run my script. But if you load in data, for example, I'm kind of cloud storage like S3 or GCP, there could be network files. Also, there is no incremental loading. So every time I load data, I would need to extract all the data and replace them into destination. And of course, it's more code to maintain. And we always want it to be less code as less possible. So again with the LTE, it might it might look like an advertising for DLT in a way it is. But because it's open source, it's such a great tool. I feel no shame. So yeah, how DLT handles a lot step automatically? Again, it's like with the real lives of code, DLT handles schema inference, every handling, incremental updates, all of the above. It also support as we already discussed quickly, multiple destinations, the full list of destination you can find in our documentation. It also optimizes for performance. It has some settings for multi-threads, multi-processes. It can be used to bashing, string, it's also schema other against a particular metal loading. And it has a little built-in mechanism for resilience and retries. So the good thing about DLT is that if load is finished, you know for sure that the data is full. There's no like missing records, there's no like half load of data, nothing of above. It's just like if load is complete, the data is there and you can trust them. Okay, how can we do it in code? Again, we need first to install DLT, but we already did it. Then in this example, we will use the decorator called DLT resource. So what it is, actually DLT thinks about the sources. With this like specific decorators called DLT resource, and it just helps you to organize your code. Basically, this resource is just like a function which extracts the data. So in this case, we can reuse our old code. Again, we're using the rest client from DLT for simple imagination. And what we're doing inside, we just iterating through these pages and just yielding the data, yielding each page. So as I said before, DLT is not only like a regular use connectors library, but it also a development tool. So with this decorator called DLT resource, you basically can build any type of resources. All you need to do inside this function is to connect to your source of the data, extract data and yield it. And then DLT will take care of all the normalization stuff, lot of things, all the data types, all this key management and stuff like this. So okay, we created a resource. Yeah, also we have the name of the resource, which is important, because it will be used as a name of the table. We iterated through all the pages and yield all the data. Then we creating the DLT pipeline with destination.db again and just running it. So as then in put here, we will pass our resource. And then DLT will iterate over this generator, because this resource is a trading generator, you see the yield. And it will go over the other records, do the three steps, extract, extract normalization a load and put data into ducted bin. Let's try it. Okay, it could take some time because we're iterating over all the pages. I think there is like 10 pages of the data bottom trove. And then at the end, we want to explore the loaded data with this data set method, which I showed you before, which gives you basically the access to the data set in the destination. And you can use the f function to get the pundit data frame. But if there is also a row function, which gives you a row data set and I think something else, you need to look at the destination. Okay, so we have here 10,000 rows of data. You can see that all the data, of course, already flattened because we have so nice API. But yeah, you can see that by applying a load step, completed in three and three seconds, one load package, the destination, WDB. Yeah, everything is loaded, contain all the failed jobs. So this is the code you need to do with DLT. And just remind me you, this is the code you need to do for only load stage for the manually without DLT. Okay, so we just loaded all the data. But as I said before, there are some several aspects we need to take care of. First of all, is incremental loading. So incremental loading is very powerful concept in data engineering. When we only want to load new or change data, we don't want each time to extract all the data and replace the entire data set. For example, if we have New York taxi, data set, we only want to load new records, new writes, which change from the last load. So it's built onto important concept. First one is incremental extraction. With this, we only extract modified data, not retrieve the whole database. And the second thing is state tracking. So we need to keep track of what was already loaded and what is not yet to ensure only new data is protest. So DLT have a mechanism for incremental loading and it actually keeps the state in the destination in separate table. So if you load your data with DLT, you will see these tables start with underscore DLT. And this is like system tables where DLT of storage state. And even each time you run the pipeline, DLT first go through the destination, extract the state, understand, okay, this one we loaded already, this one we still haven't loaded and then comes a lot. So DLT provides you two incremental loading methods, append and merge. So append is just like adding new records at the end of the table. It best used for immutable or stateless data. For example, yeah, if it's a toxic write, then probably if write is already happened, it won't be changed in the past, right? Those inutable. That is why it's good to use append with stateless example. And the merge one will update the existing records. So if it, for example, data of some clients, this would be stateful because I don't know, the email could change or the last name of client could change or someone could add some note about this client. So the data could change in the last and then in the destination, we need to merge the existing record with the new record. And this usually done based on some kind of unique, usually it's primary key, but it could be some like specific merge key for this merge. Yeah, here's the table, which shows you like where went to is better to use append when it's better to use merge. We also have similar table in our documentation, but it would be different from case to case. So in this case with New York taxi data sets, we want to load data incrementally with append. Moreover, we want to download only trips made after some specific date. In this case, it's June 15th of 2009. We'll just keep all the old ones. How can do this? Dilty has this like incremental filter and to utilize it, you need to put the initial value, which is like the date we agreed on, and then the name of the field where this like date stored. So this field to track called trip drop off daytime in this case. So each time when you run guilty by blind, Dilty will track this specific field and understand which or which data was already loaded. How can implemented in code? Very easily I will skip the install part. Again, we'll re-install it. And then what you need to do, you need to put the cursor date equal to Dilty.Sources.Incremmental, this is incremental cursor, and again specify the field to track, specify if this turned date. And that's all. All the other things similar. So this was in definition of our resource and like the pipeline, and definition and pipeline run, they haven't changed. So let's run it. This is the definition of our resource, and then let's run the pipeline. What we want to pay attention to is that now when we specify the start date, we wouldn't see all the records. So before that, let's check, we had 10,000 rows. Let's see how many records would we have right now. Why are it loading maybe I can ask a question. No. Okay, okay. I'll ask the question later. Yeah. Sure. Yeah. Here, what we're looking is how many records? Okay. So we can see normalized data for the volume tables. This is Dilty by Pine State table, which I told you about, like the system table, where the state is kept. And the right table, which has now 5000 something rows. So it's almost half as less. And if we check of what the minimal value for this trip drop of date time call, we will see that it's the date with specified. Yeah. And if we run the same pipeline again, ideally we shouldn't see any data loaded because now the load is incremental. And Dilty should check what data was loaded loaded. There's no new data added. So Dilty should be zero records. But let's see. What happens if something is deleted from the database? Does it also does it not exist? Yeah, it's very good question. So Dilty by default, not delete any data from destination because it's simply dangerous. But there are some mechanism which allow you to mirror this into destination. So by default, if a record is removed from the source, it will still be in the destination. It will not be removed. But there is a way, if you go to the load in behavior and incremental load it, there would be a way to also mirror this into destination. If you want to do hard deletes, there is also a way to do it. Yeah. But can we just mark in our in our target database, in our destination? We can just say, okay, it's deleted. Like something like a column state, deleted. Yeah, there is a way to do it. From example, there is like acidity too. I don't know if you know. Yeah, no about it. But basically it will create like a specific column called Dilty valid from and Dilty valid too. I think where it will put like the date where the record is valid too. So if it's deleted, then it's no longer valid. So yeah, we have we have a way of doing it well. Yeah, thank you. Okay. Okay, so if we go back, yeah, we run the same pipeline again and we see that zero load packages will load it, no data found to normalize, which is what we expected because we load them all in the previous run. And now that the pipeline is incremental, it's actually stateful. Cool. So yeah, now before that, we were loading data to DiltyB, which is very great, especially if you need to debug something, if you need to prototype or develop something. But in production, we of course usually load data just some kind of clouds warehouses. In this case, we will be loading data to BigQuery. Now I'll show you how easy that. First of all, you need to install Dilty with BigQuery modification because BigQuery requires like some additional libraries to be installed. Then the resource definition stays the same. Exactly the same. We don't change anything. The only thing we need to change is when we defined the pipeline, we had destination as a DiltyB. And now we need to change it to BigQuery. Also, one important note is as I said, as a beginning, if you want to connect a BigQuery, you need to have credentials from it. Here in this collab, I put my credentials in in Google Clouds, you can put them in environment variables. You can put them in dot-tombo file. So everything could be found in the documentation, how you can specify credentials with Dilty, but would call up like the easiest thing as environment variable. So to load the credentials to environment variable, I just import Google Clouds, use of data, access my key, which I specified before, and put it into the environment variable. The environment variable name is also kind of clear. So we have destination, the destination type is BigQuery, and credentials. So let's, I think, run this thing, which is the definition of resource, it's the same. Then our pipeline, we define it with BigQuery as a destination. And then let's put credentials to the environment variable. Okay, now we can run the pipeline. So again, name, destination, BigQuery. Def mode is not necessary, but you can put it. It's easier to do debugging and stuff. And let's look, okay, I have a BigQuery instance here. So if we take a look at the taxi rights, you can see here three taxi rights. This is run, so we should didn't before, now we work shop. And let's try to load it and hopefully see the first one. Okay, this could take more time because what's happening, Dilty is extraction data, normalizing it, and then connection to BigQuery and chunk by chunk and multiple threats, it load data to BigQuery. And because I don't know where BigQuery is located, yeah, it could take some time for network to process. Yeah, maybe what would happen here and how it's a different from DFTB. I showed you before that if we're doing the load manually, what it was here. So we need to create the table and the schema of the table manually. And this SQL could be very from one destination to another. So when you use a deal team, you don't need to take care how exactly can I create a table in BigQuery. How can I connect to it? Instead, you just put a BigQuery destination and Dilty will do it for you. Okay, it's loaded. So it took about around 15 seconds. Let's go to BigQuery and hopefully see the fourth one. Yeah, cool. So I don't know which one is this. The last one, on the first one. Okay, it doesn't matter. They're all the same. I guarantee you. So here we have the data. You can see the right table, which contains our actual data and three Dilty system tables where we keep track of all the loads that have been done. We keep the pipeline state, the schema and all kinds of stuff. But if you go to the right table, you can see all the columns which we saw before. So yeah, Dilty is here. Nice. Yeah, and as I mentioned before as well, it's very easy to kind of experiment and do testing and development. For example, you changing the destination to local.edu, do all of your experiments and then changing it to BigQuery back and load the data to the cloud destination. So we have only one section left and this is loading data into data lake. So as I said at the beginning, current claims and modern data engineering, there are three types of storage. Usually it's either warehouses, data lake or they houses. So we will cover here data lake. Basically in an actual, it's just like a bunch of bucket files which stored in either local file system or some cloud file system, for example, as free. It's very cost effective because it's very cheap to store a lot of files. It's optimized for big data processing. So it can easily be degrading Spark, with Databricks, with Prost-O, Databricks by the way, it's also like a lake cloud architecture and it's very easily scalable. So you can store like terabytes, better database of data efficiently. The only problem you may found here is that you need to partition the data very well to be able to find the needed data quickly. But we will cover this in a moment. So here we'll have our own small local data lake. I will just have it in my file system. In order to set up DOT for loading data to data lake, what you need to do. I'll just set up some credentials here. I will set up bucket URL. Because I'm using local file system, it's just like a folder. But if, for example, you will be using S3, then in bucket URL, where you will have something like S3, blah, blah, and then the name of your bucket. It also works for GCP, Azure worm, yeah, any kind of asyftic service, also, I think, kind of cloud storage. Then, yeah, the resource already defined. And then you need to run the pipeline. So the pipeline name here, FAS pipeline, then the destination file system. So we changed it from Bequare to file system and data set name. Yeah, so this works the same. Okay, what's for? Oh, yeah, I didn't. So yeah, this is a great example of how to debug your utility pipeline. So I can see here, following fields and missing bucket URL in configuration is not found. Okay, that is why I haven't run this cell. Let's put to the bucket URL. And let's try again. Okay, so once it would be loaded, I should find it here somewhere under the content. Right. Let's see, okay, it's loaded. Yes, here, I can see FAS data, which is the name of the data set. Inside, I can find the structure with maps, which maps my tables, which is three system tables and one table called writes and inside, I will have hopefully spark it file, yeah, here. So this is like a small data lake. And we can actually have an access to this data set with the same data set method and check if all data set is here. Yeah, they are cool all the 10,000. And as I mentioned before, for using data lakes, you need to structure them really good, because you want to be able to quickly access the data. And what helps you to do this is open force table formats like iceberg and delta tables. What they add is like a metadata layer. And with the OT is also very easy to use it. So all you need to do is pick and style the OT for pi iceberg, for example, they'll be using iceberg here. And then when you define your pipeline run, you will put again the resource, the loader 5 format is a barket, because iceberg is basically like a bunch of targets with tomato data. And then table format is iceberg. Let's see. So just yeah, it is clear mirror our open source version of DOT supports only some base basic functionality for iceberg. So you can try it out for local usage. But if you want to use it actually in like multi user with merge support with all the vendor support in et cetera, you will need to have our DOT plus version, which is like also a library, but with some additional features. Okay, so we can see here the FS iceberg data, which as I mentioned is just like a pocket file, right with a bunch of metadata. Okay, so you can use DOT for experimenting with iceberg or with delta tables as well. Okay, do we have any questions? Yeah, so as I mentioned in the module that we currently studying in the course, which is the data-grade housing module, to answer the questions in the homework, the participants need to first take a bunch of pocket files from the New York taxi data set and then load them to BigQuery. And I actually think this could be a good extra homework to do this better. This so we already have a script thanks, Michael, for putting this together. It does not use DOT, but you what you can do after this workshop is rewrite this script using DOT. And it should be shorter. Yeah, and so the question I have to you is if somebody wants to do this, but still needs a pointer. How to do this? Like, is this notebook enough or they need to look somewhere extra, maybe in the documentation to actually add on that? Yeah, so you would need to go to the documentation because this notebook covers as a source only rest API, but if you will be loading data from pocket files. By the way, where are these pocket files located? Thank you. The S3, but they serve them through CloudFront or something like that. So it's not you don't have access directly to S3. Okay, so anyway, you would need to use the file system source. Basically, oh, we basically we have really nice tutorial and maybe what I can do is I can add the link to it to our Collap. I will update it afterwards. Like somewhere here in the what's next section, for example. I can put here the link to our tutorial. And basically there you can find all the code snippets, but it will be something like this. So you will import the file system. Here it's RITCSV, but it would be a readparket, which is the same. Then you put the back at URL file warp, which would be probably just all the pocket files. Then this like reader, well you put files and readparket, and then again, DOT pipeline with the name, what we've done a several times. And as a destination, there would be a BigQuery. So it would be something like this. And if my files is a bunch of URLs, HTPS URLs, will still work. So it depends on the specific use case, but if this one is not helping, so underneath this file system, it uses FASPEC. So if a spec cannot work with it, then you just need to define your own source, which we've done here. So instead of using the ready to use source, which is file system, you will just create every source. And here you will iterate through all of your URLs, which you have, and just yield your files here. You can read them with parket and just yield them. And then if it will be like parkets. Of course, it's easy if you have a bunch of URLs, you just download them and then run this, but it would be even cooler if you don't, if you didn't need to download them. You just get them directly from the internet and save to be query without saving them. You will just iterate on them URL 1, 2, etc. Okay, so if somebody wants to do that, please create a public quest with this script. And yeah, like some of the other students can also use it. But if you want, you can just implement it yourself, play with this. Like, after you, if you want to submit a border quest or not. But I think you just did everything. Yeah, it was funny. Yeah, but it, yeah, it will require some debugging probably. Yeah, and I will put the link to the tutorial in what's next section. But yeah, again, what's next? Yeah, if you want to work more with DILT, I would recommend you to try our different destinations because it's very easy to do. Experiment with incremental loading. Like, explore how the schema relation works. And what's the most important thing is we have really nice Slack community where you can go and like ask any technical questions you have or share your progress. Like, say, hey, I've created my pet project. Take a look. So we have really nice community there and everyone's helping each other. I was muted. By the way, I put the link to this like community in the description. So you go under the video and you'll see the slinger monk, other things. So there's already a link to the workshop. There is a link to the course. There's link to the DILT, how community. And there's also a link to the 2020-24 workshop that we had last year with Adrien and his instructor. So I think they're similar, but also different. So if you want to learn more about DILT, you can also go and check that workshop. But I think like if you go to the documentation, you'll find much, much, much more. Yeah, we work on documentation constantly. It's, of course, to work in progress, the documentation always are. But yeah, I think it's very nice. Okay, so that's it. Cool. Yeah. And as I said, if you have any more questions left, you welcome to your analysis community. I now look at you and the idea lies that I've got to wear my DILT T-shirt. I'm sorry. Yeah, I'm sorry. I'm sorry. I'm sorry. I'm sorry. I like I swear. Yeah, yeah. Don't be mad at me. Okay. Yeah, thanks a lot. 90 minutes passed by. I didn't even notice. So thanks a lot that was very engaged in very helpful. And thanks everyone for joining us today for asking questions. If you have questions, DILT hops like, is the way to go? Yeah. Um, yeah, we have fun with the homework. I showed you at the beginning where you can find the homework. It's cohorts 225 workshops DILT. So this is where you will find it. Okay. So have fun. Yeah. Thank you very much for hosting this. Yeah, it was very nice to be part of it. And yeah, it's very good job you're doing with all this data units of camp. It's amazing. I really, I like meet people all the time and different conferences. And they mentioned DILT T-shirt. Oh, yeah. I saw it in the data units of camp. Yeah. Okay. Thank you. Okay. Well, have a great evening and everyone, thanks for joining us and have fun. We will have roughly one week for the, um, you will have one week for the homework. So enjoy learning DILT. Enjoy doing the homework and see you around. Bye. Bye. Bye. Bye.