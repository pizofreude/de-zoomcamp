{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pizofreude/de-zoomcamp/blob/main/workshops/misc/youtube_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjZzJ7dZg4qg"
      },
      "source": [
        "# YouTube Video Transcription with OpenAI's Whisper\n",
        "\n",
        "[![License](https://img.shields.io/github/license/kazuki-sf/youtube-whisper)](https://github.com/kazuki-sf/youtube-whisper)\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kazuki-sf/youtube-whisper/blob/main/youtube_whisper.ipynb)\n",
        "\n",
        "## How to Use the Notebook\n",
        "Feel free to `Copy to Drive` the notebook or run it directly.\n",
        "1. Enter the URL of the YouTube video or shorts you want to transcribe.\n",
        "2. Choose the whisper model you want to use.\n",
        "3. Run the code cell (Step 1-3) and wait for the transcription to complete.\n",
        "\n",
        "## Notes\n",
        "* `T4 GPU` or higher is recommended for running the notebook. You can change the runtime type by going to `Runtime` -> `Change runtime type` -> `Hardware accelerator` -> `GPU`.\n",
        "* Whenever you change the YouTube URL or Whisper Model, please run the `Step 1` and then run `Step 3` (You can skip `Step 2` if you already ran it before)\n",
        "* When you run `Step 3`, the website might ask you a permission to download multiple files.\n",
        "* This project is not affiliated with OpenAI. The code provided here is for educational purposes only.\n",
        "* Here's a list of whisper model and the relative speed of each model. For more information, please visit the official GitHub page: https://github.com/openai/whisper#available-models-and-languages\n",
        "---\n",
        "\n",
        "|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "3tgxDwewg4qm"
      },
      "outputs": [],
      "source": [
        "# @title Step 1: Enter URL & Choose Whisper Model\n",
        "\n",
        "# @markdown Enter the URL of the YouTube video\n",
        "YouTube_URL = \"https://www.youtube.com/live/pgJWP_xqO1g?si=QU96oL19GDQAmmZl\" #@param {type:\"string\"}\n",
        "\n",
        "# @markdown Choose the whisper model you want to use\n",
        "whisper_model = \"tiny\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\", \"large-v3\"]\n",
        "\n",
        "# @markdown Save the transcription as text (.txt) file?\n",
        "text = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Save the transcription as an SRT (.srt) file?\n",
        "srt = True #@param {type:\"boolean\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "BTXCzFXwg4qo",
        "outputId": "6d24afaf-807c-4652-a62a-6d5479ac0f2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Install Dependencies (this may take about 2-3 min)\n",
        "\n",
        "!pip install -q pytube\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "\n",
        "import os, re\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from pytube import YouTube\n",
        "import whisper\n",
        "from whisper.utils import get_writer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ik67jOUQg4qp",
        "outputId": "72a35107-dfc2-4173-85c6-5fea9b5d9ac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==> Downloading audio...\n",
            "\n",
            "=======================\n",
            "\n",
            "ðŸ”— YouTube URL: https://www.youtube.com/live/pgJWP_xqO1g?si=QU96oL19GDQAmmZl\n",
            "\n",
            "ðŸ¤– Whisper Model: tiny\n",
            "\n",
            "=======================\n",
            "\n",
            "==> Transcribing audio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: English\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 545436/545436 [11:34<00:00, 785.67frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==> Creating .txt file\n",
            "\n",
            "==> Creating .srt file\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2813864c-79c1-4d69-a6c9-340db8455754\", \"live_stream.txt\", 69255)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f31fb546-8b08-4784-9c9f-dea803a61f52\", \"live_stream.srt\", 96062)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ¨ All Done!\n",
            "=======================\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Transcribe the video/audio data\n",
        "!pip install -q pytubefix\n",
        "\n",
        "\n",
        "from pytubefix import YouTube\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = whisper.load_model(whisper_model).to(device)\n",
        "\n",
        "# Util function to change name\n",
        "def to_snake_case(name):\n",
        "    return name.lower().replace(\" \", \"_\").replace(\":\", \"_\").replace(\"__\", \"_\")\n",
        "\n",
        "# Download the audio data from YouTube video\n",
        "\n",
        "def download_audio_from_youtube(url, file_name=None, out_dir=\".\"):\n",
        "    print(f\"\\n==> Downloading audio...\")\n",
        "    yt = YouTube(url)\n",
        "    # Check if the video is a live stream\n",
        "    if yt.streaming_data:\n",
        "        # If it's a live stream, use a default filename\n",
        "        file_name = Path(out_dir, \"live_stream\").with_suffix(\".mp4\")\n",
        "    else:\n",
        "        # If it's not a live stream, use the video title for the filename\n",
        "        if file_name is None:\n",
        "            file_name = Path(out_dir, to_snake_case(yt.title)).with_suffix(\".mp4\")\n",
        "    yt = (yt.streams\n",
        "            .filter(only_audio=True, file_extension=\"mp4\")\n",
        "            .order_by(\"abr\")\n",
        "            .desc())\n",
        "    # Convert the PosixPath object to a string before passing it to download\n",
        "    return yt.first().download(filename=str(file_name))\n",
        "\n",
        "\n",
        "# Transcribe the audio data with Whisper\n",
        "def transcribe_audio(model, file, text, srt):\n",
        "    print(\"\\n=======================\")\n",
        "    print(f\"\\nðŸ”— YouTube URL: {YouTube_URL}\")\n",
        "    print(f\"\\nðŸ¤– Whisper Model: {whisper_model}\")\n",
        "    print(\"\\n=======================\")\n",
        "\n",
        "    file_path = Path(file)\n",
        "    output_directory = file_path.parent\n",
        "\n",
        "    # Run Whisper to transcribe audio\n",
        "    print(f\"\\n==> Transcribing audio\")\n",
        "    result = model.transcribe(file, verbose = False)\n",
        "\n",
        "    if text:\n",
        "        print(f\"\\n==> Creating .txt file\")\n",
        "        txt_path = file_path.with_suffix(\".txt\")\n",
        "        with open(txt_path, \"w\", encoding=\"utf-8\") as txt:\n",
        "            txt.write(result[\"text\"])\n",
        "    if srt:\n",
        "        print(f\"\\n==> Creating .srt file\")\n",
        "        srt_writer = get_writer(\"srt\", output_directory)\n",
        "        srt_writer(result, str(file_path.stem))\n",
        "\n",
        "    # Download the transcribed files locally\n",
        "    from google.colab import files\n",
        "\n",
        "    colab_files = Path(\"/content\")\n",
        "    stem = file_path.stem\n",
        "\n",
        "    for colab_file in colab_files.glob(f\"{stem}*\"):\n",
        "        if colab_file.suffix in [\".txt\", \".srt\"]:\n",
        "            files.download(str(colab_file))\n",
        "\n",
        "    print(\"\\nâœ¨ All Done!\")\n",
        "    print(\"=======================\")\n",
        "    return result\n",
        "\n",
        "# Download & Transcribe the audio data\n",
        "audio = download_audio_from_youtube(YouTube_URL)\n",
        "result = transcribe_audio(model, audio, text, srt)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}